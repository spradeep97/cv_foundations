{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of EECS 504 PS4: Backpropagation",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "127f4d92af0e4da8af078128a951e542": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_65e60e9eef6d4af7a79897348a5484f0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2862ddfc88124249bbf96a3b86e9c5e8",
              "IPY_MODEL_8a665f4484d043e888b7e46cc1d7f453"
            ]
          }
        },
        "65e60e9eef6d4af7a79897348a5484f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2862ddfc88124249bbf96a3b86e9c5e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6b4b0ea57e2c4a2d912467b8c8e1fa67",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_696841c613a241a4a950710c30ae421e"
          }
        },
        "8a665f4484d043e888b7e46cc1d7f453": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_36bbdc233c8f4ff8a4e72db90c0678d5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "170500096it [00:30, 17107566.18it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_15cd19ee57af4a06b13fa46dc16d100f"
          }
        },
        "6b4b0ea57e2c4a2d912467b8c8e1fa67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "696841c613a241a4a950710c30ae421e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "36bbdc233c8f4ff8a4e72db90c0678d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "15cd19ee57af4a06b13fa46dc16d100f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spradeep97/cv_foundations/blob/master/4_Backpropagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ix5dQS2rUMlu",
        "colab_type": "text"
      },
      "source": [
        "#EECS 504 PS4: Backpropagation\n",
        "\n",
        "Please provide the following information \n",
        "(e.g. Andrew Owens, ahowens):\n",
        "\n",
        "Shreyas Bhat, shreyasb\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_Cst4k4tuBc",
        "colab_type": "text"
      },
      "source": [
        "# Starting\n",
        "\n",
        "Run the following code to import the modules you'll need. After your finish the assignment, remember to run all cells and save the note book to your local machine as a .ipynb file for Canvas submission."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHumIO-xt57H",
        "colab_type": "code",
        "outputId": "55419ede-8965-4769-f12e-585978eb2451",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "127f4d92af0e4da8af078128a951e542",
            "65e60e9eef6d4af7a79897348a5484f0",
            "2862ddfc88124249bbf96a3b86e9c5e8",
            "8a665f4484d043e888b7e46cc1d7f453",
            "6b4b0ea57e2c4a2d912467b8c8e1fa67",
            "696841c613a241a4a950710c30ae421e",
            "36bbdc233c8f4ff8a4e72db90c0678d5",
            "15cd19ee57af4a06b13fa46dc16d100f"
          ]
        }
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import scipy\n",
        "from torchvision.datasets import CIFAR10\n",
        "download = not os.path.isdir('cifar-10-batches-py')\n",
        "dset_train = CIFAR10(root='.', download=download)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "127f4d92af0e4da8af078128a951e542",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./cifar-10-python.tar.gz to .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apEPzDNtK0MC",
        "colab_type": "text"
      },
      "source": [
        "# Problem 4.2 Multi-layer perceptron\n",
        "In this problem you will develop a two Layer neural network with fully-connected layers to perform classification, and test it out on the CIFAR-10 dataset.\n",
        "\n",
        "We train the network with a softmax loss function on the weight matrices. The network uses a ReLU nonlinearity after the first fully connected layer. In other words, the network has the following architecture:\n",
        "\n",
        "input - fully connected layer - ReLU - fully connected layer - softmax\n",
        "\n",
        "The outputs of the second fully-connected layer are the scores for each class.\n",
        "\n",
        "You cannot use any deep learning libraries such as PyTorch in this part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXfumCQ21JoK",
        "colab_type": "text"
      },
      "source": [
        "# 4.2 (a) Layers\n",
        "In this problem, implement fully connected layer, relu and softmax. Filling in all TODOs in skeleton codes will be sufficient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-ljfgMv9PHx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fc_forward(x, w, b):\n",
        "    \"\"\"\n",
        "    Computes the forward pass for a fully-connected layer.\n",
        "    \n",
        "    The input x has shape (N, Din) and contains a minibatch of N\n",
        "    examples, where each example x[i] has shape (Din,).\n",
        "    \n",
        "    Inputs:\n",
        "    - x: A numpy array containing input data, of shape (N, Din)\n",
        "    - w: A numpy array of weights, of shape (Din, Dout)\n",
        "    - b: A numpy array of biases, of shape (Dout,)\n",
        "    \n",
        "    Returns a tuple of:\n",
        "    - out: output, of shape (N, Dout)\n",
        "    - cache: (x, w, b)\n",
        "    \"\"\"\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the forward pass. Store the result in out.              #\n",
        "    ###########################################################################\n",
        "    out = np.matmul(x,w)+b\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    cache = (x, w, b)\n",
        "    return out, cache\n",
        "\n",
        "\n",
        "def fc_backward(dout, cache):\n",
        "    \"\"\"\n",
        "    Computes the backward pass for a fully_connected layer.\n",
        "    \n",
        "    Inputs:\n",
        "    - dout: Upstream derivative, of shape (N, Dout)\n",
        "    - cache: returned by your forward function. Tuple of:\n",
        "      - x: Input data, of shape (N, Din)\n",
        "      - w: Weights, of shape (Din, Dout)\n",
        "      - b: Biases, of shape (Dout,)\n",
        "      \n",
        "    Returns a tuple of:\n",
        "    - dx: Gradient with respect to x, of shape (N, Din)\n",
        "    - dw: Gradient with respect to w, of shape (Din, Dout)\n",
        "    - db: Gradient with respect to b, of shape (Dout,)\n",
        "    \"\"\"\n",
        "    x, w, b = cache\n",
        "    dx, dw, db = None, None, None\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the affine backward pass.                               #\n",
        "    ###########################################################################\n",
        "    dx = dout @ np.transpose(w)\n",
        "    dw = np.transpose(x) @ dout\n",
        "    db = np.transpose(dout) @ np.ones(dout.shape[0])\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    return dx, dw, db\n",
        "\n",
        "def relu_forward(x):\n",
        "    \"\"\"\n",
        "    Computes the forward pass for a layer of rectified linear units (ReLUs).\n",
        "\n",
        "    Input:\n",
        "    - x: Inputs, of any shape\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - out: Output, of the same shape as x\n",
        "    - cache: x\n",
        "    \"\"\"\n",
        "    out = x\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the ReLU forward pass.                                  #\n",
        "    ###########################################################################\n",
        "    out = x * (x >= 0)\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    cache = x\n",
        "    return out, cache\n",
        "\n",
        "\n",
        "def relu_backward(dout, cache):\n",
        "    \"\"\"\n",
        "    Computes the backward pass for a layer of rectified linear units (ReLUs).\n",
        "\n",
        "    Input:\n",
        "    - dout: Upstream derivatives, of any shape\n",
        "    - cache: returned by your forward function. Input x, of same shape as dout\n",
        "\n",
        "    Returns:\n",
        "    - dx: Gradient with respect to x\n",
        "    \"\"\"\n",
        "    dx, x = dout, cache\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the ReLU backward pass.                                 #\n",
        "    ###########################################################################\n",
        "    dx = dout * (x >= 0)\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    return dx\n",
        "\n",
        "\n",
        "def softmax_loss(x, y):\n",
        "    \"\"\"\n",
        "    Computes the loss and gradient for softmax classification.\n",
        "\n",
        "    Inputs:\n",
        "    - x: Input data, of shape (N, C) where x[i, j] is the score for the jth\n",
        "      class for the ith input.\n",
        "    - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and\n",
        "      0 <= y[i] < C\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - loss: Scalar giving the loss\n",
        "    - dx: Gradient of the loss with respect to x\n",
        "    \"\"\"\n",
        "    loss, dx = None, None\n",
        "    ###########################################################################\n",
        "    # TODO: Implement softmax loss                                            #\n",
        "    ###########################################################################\n",
        "    loss = 0.0\n",
        "    #print(x.shape, y.shape)\n",
        "    scores = np.exp(x.T-np.max(x,axis=1))\n",
        "    scores /= scores.sum(axis=0)\n",
        "    scores = np.transpose(scores)        \n",
        "\n",
        "    one_hot = np.zeros_like(scores)\n",
        "    #print(y.shape, scores.shape)\n",
        "    for i in range(len(y)):\n",
        "      one_hot[i,y[i]] = 1\n",
        "\n",
        "    for i in range(x.shape[0]):\n",
        "      loss += np.sum(-one_hot[i]*np.log(scores[i]))\n",
        "    loss /= x.shape[0]\n",
        "    dx = np.transpose(scores - one_hot) / x.shape[0]\n",
        "    dx = dx.T\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    return loss, dx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbFxtS3zK8oz",
        "colab_type": "text"
      },
      "source": [
        "# 4.2 (b) Softmax Classifier\n",
        "\n",
        "In this problem, implement softmax classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytvxbx9UpxVL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SoftmaxClassifier(object):\n",
        "    \"\"\"\n",
        "    A fully-connected neural network with\n",
        "    softmax loss that uses a modular layer design. We assume an input dimension\n",
        "    of D, a hidden dimension of H, and perform classification over C classes.\n",
        "\n",
        "    The architecture should be fc - relu - fc - softmax with one hidden layer\n",
        "\n",
        "    The learnable parameters of the model are stored in the dictionary\n",
        "    self.params that maps parameter names to numpy arrays.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim=3072, hidden_dim=300, num_classes=10,\n",
        "                 weight_scale=1e-3):\n",
        "        \"\"\"\n",
        "        Initialize a new network.\n",
        "\n",
        "        Inputs:\n",
        "        - input_dim: An integer giving the size of the input\n",
        "        - hidden_dim: An integer giving the size of the hidden layer, None\n",
        "          if there's no hidden layer.\n",
        "        - num_classes: An integer giving the number of classes to classify\n",
        "        - weight_scale: Scalar giving the standard deviation for random\n",
        "          initialization of the weights.\n",
        "        \"\"\"\n",
        "        self.params = {}\n",
        "        ############################################################################\n",
        "        # TODO: Initialize the weights and biases of the two-layer net. Weights    #\n",
        "        # should be initialized from a Gaussian centered at 0.0 with               #\n",
        "        # standard deviation equal to weight_scale, and biases should be           #\n",
        "        # initialized to zero. All weights and biases should be stored in the      #\n",
        "        # dictionary self.params, with fc weights and biases using the keys        #\n",
        "        # 'W' and 'b', i.e., W1, b1 for the weights and bias in the first linear   #\n",
        "        # layer, W2, b2 for the weights and bias in the second linear layer.       #\n",
        "        ############################################################################\n",
        "        self.hidden_dim = hidden_dim\n",
        "        W1_shape = (input_dim, hidden_dim)\n",
        "        W1 = np.random.normal(loc=0.0,scale=weight_scale,size=W1_shape)\n",
        "        b1 = np.zeros(hidden_dim)\n",
        "        W2_shape = (hidden_dim, num_classes)\n",
        "        W2 = np.random.normal(loc=0.0,scale=weight_scale,size=W2_shape)\n",
        "        #print(\"W2 shape\", W2_shape)\n",
        "        b2 = np.zeros(num_classes)\n",
        "        self.params['W1'] = W1\n",
        "        self.params['b1'] = b1\n",
        "        self.params['W2'] = W2\n",
        "        self.params['b2'] = b2\n",
        "        ############################################################################\n",
        "        #                             END OF YOUR CODE                             #\n",
        "        ############################################################################\n",
        "\n",
        "\n",
        "    def forwards_backwards(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Compute loss and gradient for a minibatch of data.\n",
        "\n",
        "        Inputs:\n",
        "        - X: Array of input data of shape (N, Din)\n",
        "        - y: Array of labels, of shape (N,). y[i] gives the label for X[i].\n",
        "\n",
        "        Returns:\n",
        "        If y is None, then run a test-time forward pass of the model and return:\n",
        "        - scores: Array of shape (N, C) giving classification scores, where\n",
        "          scores[i, c] is the classification score for X[i] and class c.\n",
        "\n",
        "        If y is not None, then run a training-time forward and backward pass. And\n",
        "        return a tuple of:\n",
        "        - loss: Scalar value giving the loss\n",
        "        - grads: Dictionary with the same keys as self.params, mapping parameter\n",
        "          names to gradients of the loss with respect to those parameters.\n",
        "        \"\"\"\n",
        "        scores = None\n",
        "        ############################################################################\n",
        "        # TODO: Implement the forward pass for the two-layer net, computing the    #\n",
        "        # class scores for X and storing them in the scores variable.              #\n",
        "        ############################################################################\n",
        "        \n",
        "        x1, cache1 = fc_forward(X, self.params['W1'], self.params['b1'])\n",
        "        x2, x1 = relu_forward(x1)\n",
        "        scores, cache2 = fc_forward(x2, self.params['W2'],self.params['b2'])\n",
        "        \n",
        "        \n",
        "        ############################################################################\n",
        "        #                             END OF YOUR CODE                             #\n",
        "        ############################################################################\n",
        "\n",
        "        # If y is None then we are in test mode so just return scores\n",
        "        if y is None:\n",
        "            return scores\n",
        "\n",
        "        loss, grads = 0, {}\n",
        "        ############################################################################\n",
        "        # TODO: Implement the backward pass for the two-layer net. Store the loss  #\n",
        "        # in the loss variable and gradients in the grads dictionary. Compute data #\n",
        "        # loss using softmax, and make sure that grads[k] holds the gradients for  #\n",
        "        # self.params[k].                                                          # \n",
        "        ############################################################################\n",
        "        loss, loss_scores = softmax_loss(scores, y)\n",
        "        (dx2,dw2,db2) = fc_backward(loss_scores, cache2)\n",
        "        loss_x1 = relu_backward(dx2, x1)\n",
        "        (dx1,dw1,db1) = fc_backward(loss_x1, cache1)\n",
        "\n",
        "        #print(dx1.shape, dw1.shape, db1.shape)\n",
        "        #print(dx2.shape, dw2.shape, db2.shape)\n",
        "\n",
        "        grads['W1'] = dw1\n",
        "        grads['b1'] = db1\n",
        "        grads['W2'] = dw2\n",
        "        grads['b2'] = db2\n",
        "        ############################################################################\n",
        "        #                             END OF YOUR CODE                             #\n",
        "        ############################################################################\n",
        "        return loss, grads\n",
        "\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwp0waIL1h_e",
        "colab_type": "text"
      },
      "source": [
        "# 4.2(c) Training\n",
        "\n",
        "In this problem, you need to preprocess the images and set up model hyperparameters. Notice that adjust the training and val split is optional."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZPtQzXGMoCg",
        "colab_type": "code",
        "outputId": "0d34cf59-a9e8-4564-a03b-bbba8cb58d36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding=\"latin1\")\n",
        "    return dict\n",
        "\n",
        "def load_cifar10():\n",
        "    data = {}\n",
        "    meta = unpickle(\"cifar-10-batches-py/batches.meta\")\n",
        "    batch1 = unpickle(\"cifar-10-batches-py/data_batch_1\")\n",
        "    batch2 = unpickle(\"cifar-10-batches-py/data_batch_2\")\n",
        "    batch3 = unpickle(\"cifar-10-batches-py/data_batch_3\")\n",
        "    batch4 = unpickle(\"cifar-10-batches-py/data_batch_4\")\n",
        "    batch5 = unpickle(\"cifar-10-batches-py/data_batch_5\")\n",
        "    test_batch = unpickle(\"cifar-10-batches-py/test_batch\")\n",
        "    X_train = np.vstack((batch1['data'], batch2['data'], batch3['data'],\\\n",
        "                         batch4['data'], batch5['data']))\n",
        "    Y_train = np.array(batch1['labels'] + batch2['labels'] + batch3['labels'] + \n",
        "                       batch4['labels'] + batch5['labels'])\n",
        "    X_test = test_batch['data']\n",
        "    Y_test = test_batch['labels']\n",
        "    \n",
        "    #Preprocess images here                                     \n",
        "    X_train = (X_train-np.mean(X_train,axis=1,keepdims=True))/np.std(X_train,axis=1,keepdims=True)\n",
        "    X_test = (X_test-np.mean(X_test,axis=1,keepdims=True))/np.std(X_test,axis=1,keepdims=True)\n",
        "\n",
        "    data['X_train'] = X_train[:40000]\n",
        "    data['y_train'] = Y_train[:40000]\n",
        "    data['X_val'] = X_train[40000:]\n",
        "    data['y_val'] = Y_train[40000:]\n",
        "    data['X_test'] = X_test\n",
        "    data['y_test'] = Y_test\n",
        "    return data\n",
        "\n",
        "def test_network(model, X, y, num_samples=None, batch_size=100):\n",
        "    \"\"\"\n",
        "    Check accuracy of the model on the provided data.\n",
        "\n",
        "    Inputs:\n",
        "    - model: Image classifier\n",
        "    - X: Array of data, of shape (N, d_1, ..., d_k)\n",
        "    - y: Array of labels, of shape (N,)\n",
        "    - num_samples: If not None, subsample the data and only test the model\n",
        "      on num_samples datapoints.\n",
        "    - batch_size: Split X and y into batches of this size to avoid using\n",
        "      too much memory.\n",
        "\n",
        "    Returns:\n",
        "    - acc: Scalar giving the fraction of instances that were correctly\n",
        "      classified by the model.\n",
        "    \"\"\"\n",
        "\n",
        "    # Subsample the data\n",
        "    N = X.shape[0]\n",
        "    if num_samples is not None and N > num_samples:\n",
        "        mask = np.random.choice(N, num_samples)\n",
        "        N = num_samples\n",
        "        X = X[mask]\n",
        "        y = y[mask]\n",
        "\n",
        "    # Compute predictions in batches\n",
        "    num_batches = N // batch_size\n",
        "    if N % batch_size != 0:\n",
        "        num_batches += 1\n",
        "    y_pred = []\n",
        "    for i in range(num_batches):\n",
        "        start = i * batch_size\n",
        "        end = (i + 1) * batch_size\n",
        "        scores = model.forwards_backwards(X[start:end])\n",
        "        y_pred.append(np.argmax(scores, axis=1))\n",
        "    y_pred = np.hstack(y_pred)\n",
        "    acc = np.mean(y_pred == y)\n",
        "\n",
        "    return acc\n",
        "\n",
        "\n",
        "def train_network(model, data, **kwargs):\n",
        "    \"\"\"\n",
        "     Required arguments:\n",
        "    - model: Image classifier\n",
        "    - data: A dictionary of training and validation data containing:\n",
        "      'X_train': Array, shape (N_train, d_1, ..., d_k) of training images\n",
        "      'X_val': Array, shape (N_val, d_1, ..., d_k) of validation images\n",
        "      'y_train': Array, shape (N_train,) of labels for training images\n",
        "      'y_val': Array, shape (N_val,) of labels for validation images\n",
        "\n",
        "    Optional arguments:\n",
        "    - learning_rate: A scalar for initial learning rate.\n",
        "    - lr_decay: A scalar for learning rate decay; after each epoch the\n",
        "      learning rate is multiplied by this value.\n",
        "    - batch_size: Size of minibatches used to compute loss and gradient\n",
        "      during training.\n",
        "    - num_epochs: The number of epochs to run for during training.\n",
        "    - print_every: Integer; training losses will be printed every\n",
        "      print_every iterations.\n",
        "    - verbose: Boolean; if set to false then no output will be printed\n",
        "      during training.\n",
        "    - num_train_samples: Number of training samples used to check training\n",
        "      accuracy; default is 1000; set to None to use entire training set.\n",
        "    - num_val_samples: Number of validation samples to use to check val\n",
        "      accuracy; default is None, which uses the entire validation set.\n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    learning_rate =  kwargs.pop('learning_rate', 1e-3)\n",
        "    lr_decay = kwargs.pop('lr_decay', 1.0)\n",
        "    batch_size = kwargs.pop('batch_size', 100)\n",
        "    num_epochs = kwargs.pop('num_epochs', 10)\n",
        "    num_train_samples = kwargs.pop('num_train_samples', 1000)\n",
        "    num_val_samples = kwargs.pop('num_val_samples', None)\n",
        "    print_every = kwargs.pop('print_every', 10)   \n",
        "    verbose = kwargs.pop('verbose', True)\n",
        "    \n",
        "    epoch = 0\n",
        "    best_val_acc = 0\n",
        "    best_params = {}\n",
        "    loss_history = []\n",
        "    train_acc_history = []\n",
        "    val_acc_history = []\n",
        "    \n",
        "    \n",
        "    num_train = data['X_train'].shape[0]\n",
        "    iterations_per_epoch = max(num_train // batch_size, 1)\n",
        "    num_iterations = num_epochs * iterations_per_epoch\n",
        "    \n",
        "\n",
        "    \n",
        "    for t in range(num_iterations):\n",
        "        # Make a minibatch of training data\n",
        "        batch_mask = np.random.choice(num_train, batch_size)\n",
        "        X_batch = data['X_train'][batch_mask]\n",
        "        y_batch = data['y_train'][batch_mask]\n",
        "        \n",
        "        # Compute loss and gradient\n",
        "        loss, grads = model.forwards_backwards(X_batch, y_batch)\n",
        "        loss_history.append(loss)\n",
        "\n",
        "        # Perform a parameter update\n",
        "        for p, w in model.params.items():\n",
        "            #print(p)\n",
        "            model.params[p] = w - grads[p]*learning_rate\n",
        "          \n",
        "        # Print training loss\n",
        "        if verbose and t % print_every == 0:\n",
        "            print('(Iteration %d / %d) loss: %f' % (\n",
        "                   t + 1, num_iterations, loss_history[-1]))\n",
        "         \n",
        "        # At the end of every epoch, increment the epoch counter and decay\n",
        "        # the learning rate.\n",
        "        epoch_end = (t + 1) % iterations_per_epoch == 0\n",
        "        if epoch_end:\n",
        "            epoch += 1\n",
        "            learning_rate *= lr_decay\n",
        "        \n",
        "        # Check train and val accuracy on the first iteration, the last\n",
        "        # iteration, and at the end of each epoch.\n",
        "        first_it = (t == 0)\n",
        "        last_it = (t == num_iterations - 1)\n",
        "        if first_it or last_it or epoch_end:\n",
        "            train_acc = test_network(model, data['X_train'], data['y_train'],\n",
        "                num_samples= num_train_samples)\n",
        "            val_acc = test_network(model, data['X_val'], data['y_val'],\n",
        "                num_samples=num_val_samples)\n",
        "            train_acc_history.append(train_acc)\n",
        "            val_acc_history.append(val_acc)\n",
        "\n",
        "            if verbose:\n",
        "                print('(Epoch %d / %d) train acc: %f; val_acc: %f' % (\n",
        "                       epoch, num_epochs, train_acc, val_acc))\n",
        "\n",
        "            # Keep track of the best model\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                best_params = {}\n",
        "                for k, v in model.params.items():\n",
        "                    best_params[k] = v.copy()\n",
        "        \n",
        "    model.params = best_params\n",
        "        \n",
        "    return model, train_acc_history, val_acc_history\n",
        "        \n",
        "\n",
        "# load data\n",
        "data = load_cifar10() \n",
        "train_data = { k: data[k] for k in ['X_train', 'y_train', \n",
        "                                    'X_val', 'y_val']}\n",
        "#######################################################################\n",
        "# TODO: Set up model hyperparameters                                  #\n",
        "#######################################################################\n",
        "\n",
        "# initialize model\n",
        "#model = SoftmaxClassifier(hidden_dim =50, weight_scale=1e-2)\n",
        "model = SoftmaxClassifier()\n",
        "# start training    \n",
        "model, train_acc_history, val_acc_history = train_network(\n",
        "    model, train_data, learning_rate = 0.02,\n",
        "    lr_decay=1.0, num_epochs=10, \n",
        "    batch_size=100, print_every=1000)\n",
        "#######################################################################\n",
        "#                         END OF YOUR CODE                            #\n",
        "#######################################################################\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(Iteration 1 / 4000) loss: 2.302579\n",
            "(Epoch 0 / 10) train acc: 0.106000; val_acc: 0.106900\n",
            "(Epoch 1 / 10) train acc: 0.373000; val_acc: 0.352800\n",
            "(Epoch 2 / 10) train acc: 0.451000; val_acc: 0.411700\n",
            "(Iteration 1001 / 4000) loss: 1.599479\n",
            "(Epoch 3 / 10) train acc: 0.443000; val_acc: 0.438100\n",
            "(Epoch 4 / 10) train acc: 0.505000; val_acc: 0.457900\n",
            "(Epoch 5 / 10) train acc: 0.496000; val_acc: 0.474500\n",
            "(Iteration 2001 / 4000) loss: 1.422046\n",
            "(Epoch 6 / 10) train acc: 0.563000; val_acc: 0.490100\n",
            "(Epoch 7 / 10) train acc: 0.535000; val_acc: 0.493200\n",
            "(Iteration 3001 / 4000) loss: 1.125070\n",
            "(Epoch 8 / 10) train acc: 0.613000; val_acc: 0.503800\n",
            "(Epoch 9 / 10) train acc: 0.593000; val_acc: 0.502000\n",
            "(Epoch 10 / 10) train acc: 0.610000; val_acc: 0.506800\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcovGmpXvXXa",
        "colab_type": "text"
      },
      "source": [
        "# 4.2(c) Report Accuracy\n",
        "\n",
        "Run the given code and report the accuracy on test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwCq8pBhu6dz",
        "colab_type": "code",
        "outputId": "95d6a180-c323-458f-f530-034f3f23b6bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# report test accuracy\n",
        "acc = test_network(model, data['X_test'], data['y_test'])\n",
        "print(\"Test accuracy: {}\".format(acc))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.5035\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTrmbULS7i2N",
        "colab_type": "text"
      },
      "source": [
        "# 4.2(d) Plot\n",
        "\n",
        "Using the train_acc_history and val_acc_history, plot the train & val accuracy versus epochs on one plot. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPjtnbya9S7g",
        "colab_type": "code",
        "outputId": "28e9f1ea-6a54-4564-f9d6-31878c664d2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "plt.figure()\n",
        "plt.plot(train_acc_history)\n",
        "plt.plot(val_acc_history)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(labels=['training', 'validation'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fa3d1a36668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxMzTLnXt8wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiQKD1UrFkX-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}